{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBdMLyAGyOVeZlM8dZ5vtN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MEnisSen/indian_pines_cnn_tf/blob/main/HSI_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DATA**"
      ],
      "metadata": {
        "id": "ncHA7DtbbLXK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krAeQwXabK7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Basic CNN**"
      ],
      "metadata": {
        "id": "uLl-C6rBiqJA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyobFarFiLtq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_pad(X, pad):\n",
        "  return np.pad(X, ((0, 0),(pad, pad),(pad, pad),(0, 0)), 'constant', constant_values=0)"
      ],
      "metadata": {
        "id": "MFkuwKAWi_S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_single_step(a_slice_prev, W, b):\n",
        "  return np.sum(a_slice_prev * W) + float(b)"
      ],
      "metadata": {
        "id": "Je0Fs5oGjKq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_forward(A_prev, W, b, hyperparameters):\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  (f, f, n_C_prev, n_C) = W.shape\n",
        "  stride = hyperparameters[\"stride\"]\n",
        "  pad = hyperparameters[\"pad\"]\n",
        "\n",
        "  n_H = ((n_H_prev-f+2*pad)/stride)+1\n",
        "  n_W = ((n_W_prev-f+2*pad)/stride)+1\n",
        "\n",
        "  Z = np.zeros((int(m), int(n_H), int(n_W), int(n_C)))\n",
        "  A_prev_pad = zero_pad(A_prev, pad)\n",
        "\n",
        "  for i in range(m):\n",
        "    a_prev_pad = A_prev_pad[i]\n",
        "    for h in range(int(n_H)):\n",
        "      vert_start = h*stride\n",
        "      vert_end = vert_start+f\n",
        "          \n",
        "      for w in range(int(n_W)):\n",
        "        horiz_start = w*stride\n",
        "        horiz_end = horiz_start+f\n",
        "              \n",
        "        for c in range(int(n_C)):\n",
        "          a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "                  \n",
        "          weights = W[:, :, :, c]\n",
        "          biases  = b[:, :, :, c]\n",
        "          Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
        "\n",
        "  cache = (A_prev, W, b, hyperparameters)\n",
        "  return Z, cache"
      ],
      "metadata": {
        "id": "GjRpX9akkJYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_forward(A_prev, hyperparameters, mode = 'max'):\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  f = hyperparameters[\"f\"]\n",
        "  stride = hyperparameters[\"stride\"]\n",
        "\n",
        "  n_H = int(1 + (n_H_prev - f) / stride)\n",
        "  n_W = int(1 + (n_W_prev - f) / stride)\n",
        "  n_C = n_C_prev\n",
        "\n",
        "  A = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "  for i in range(m):\n",
        "    a_prev_slice = A_prev[i]\n",
        "    for h in range(n_H):\n",
        "      vert_start = stride * h \n",
        "      vert_end = vert_start  + f\n",
        "            \n",
        "      for w in range(n_W):\n",
        "        horiz_start = stride * w\n",
        "        horiz_end = horiz_start + f\n",
        "                \n",
        "        for c in range(n_C):\n",
        "          a_slice_prev = a_prev_slice[vert_start:vert_end,horiz_start:horiz_end,c]\n",
        "                    \n",
        "          if mode == \"max\":\n",
        "            A[i, h, w, c] = np.max(a_slice_prev)\n",
        "          elif mode == \"average\":\n",
        "            A[i, h, w, c] = np.mean(a_slice_prev)\n",
        "\n",
        "  cache = (A_prev, hyperparameters)\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "JfrTjExjmxaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dZ, cache):\n",
        "  (A_prev, W, b, hparameters) = cache\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  (f, f, n_C_prev, n_C) = W.shape\n",
        "  stride = hparameters[\"stride\"]\n",
        "  pad = hparameters[\"pad\"]\n",
        "  (m, n_H, n_W, n_C) = dZ.shape\n",
        "    \n",
        "  dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
        "  dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "  db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "  A_prev_pad = zero_pad(A_prev, pad)\n",
        "  dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "  \n",
        "  for i in range(m):\n",
        "    a_prev_pad = A_prev_pad[i]\n",
        "    da_prev_pad = dA_prev_pad[i]\n",
        "    \n",
        "    for h in range(n_H):\n",
        "      for w in range(n_W):\n",
        "        for c in range(n_C):\n",
        "          vert_start = h * stride\n",
        "\n",
        "          vert_end = vert_start + f\n",
        "          horiz_start = w * stride\n",
        "\n",
        "          horiz_end = horiz_start + f\n",
        "\n",
        "          a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "          da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "          dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "          db[:,:,:,c] += dZ[i, h, w, c]\n",
        "\n",
        "      dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "    \n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "MOLPsp3DnpuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_window(x):\n",
        "  return x == np.max(x)"
      ],
      "metadata": {
        "id": "wvXy6DD2ovYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distribute_value(dz, shape):\n",
        "    (n_H, n_W) = shape\n",
        "    return np.ones(shape) * (dz / (n_H * n_W))"
      ],
      "metadata": {
        "id": "YnXTN65ppBl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "  (A_prev, hparameters) = cache\n",
        "  stride = hparameters[\"stride\"]\n",
        "  f = hparameters[\"f\"]\n",
        "  \n",
        "  m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "  m, n_H, n_W, n_C = dA.shape\n",
        "  \n",
        "  dA_prev = np.zeros(A_prev.shape)\n",
        "  \n",
        "  for i in range(m):\n",
        "    a_prev = A_prev[i]\n",
        "    for h in range(n_H):\n",
        "      for w in range(n_W):\n",
        "        for c in range(n_C):\n",
        "          vert_start = h\n",
        "          vert_end = vert_start + f\n",
        "          horiz_start = w\n",
        "          horiz_end = horiz_start + f\n",
        "          \n",
        "          if mode == \"max\":\n",
        "            a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "            mask = create_mask_from_window(a_prev_slice)\n",
        "            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
        "              \n",
        "          elif mode == \"average\":\n",
        "            da = dA[i, h, w, c]\n",
        "            shape = (f, f)\n",
        "            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
        "  \n",
        "  return dA_prev"
      ],
      "metadata": {
        "id": "w7S7uotLpVoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YySgjMnp3Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TENSORFLOW - Unet**"
      ],
      "metadata": {
        "id": "MbfG_5UdbJbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################ https://keras.io/examples/vision/3D_image_classification/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PKLfwAyZbT0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, Dropout, Conv3DTranspose, concatenate"
      ],
      "metadata": {
        "id": "lDPE3L2Ccgw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data Load/Split*"
      ],
      "metadata": {
        "id": "UTtqVMeIbytX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load and Split\n",
        "path = ''\n",
        "image_path = os.path.join(path, './data/CameraRGB/')\n",
        "mask_path = os.path.join(path, './data/CameraMask/')\n",
        "image_list = os.listdir(image_path)\n",
        "mask_list = os.listdir(mask_path)\n",
        "image_list = [image_path+i for i in image_list]\n",
        "mask_list = [mask_path+i for i in mask_list]\n",
        "\n",
        "image_list_ds = tf.data.Dataset.list_files(image_list, shuffle=False)\n",
        "mask_list_ds = tf.data.Dataset.list_files(mask_list, shuffle=False)\n",
        "\n",
        "#for path in zip(image_list_ds.take(3), mask_list_ds.take(3)):\n",
        "#    print(path)\n",
        "\n",
        "image_filenames = tf.constant(image_list)\n",
        "masks_filenames = tf.constant(mask_list)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((image_filenames, masks_filenames))\n",
        "\n",
        "#for image, mask in dataset.take(1):\n",
        "#    print(image)\n",
        "#    print(mask)"
      ],
      "metadata": {
        "id": "oAp-T6pRbxM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check image and labeled\n",
        "N = 2\n",
        "img = imageio.imread(image_list[N])\n",
        "mask = imageio.imread(mask_list[N])\n",
        "#mask = np.array([max(mask[i, j]) for i in range(mask.shape[0]) for j in range(mask.shape[1])]).reshape(img.shape[0], img.shape[1])\n",
        "\n",
        "fig, arr = plt.subplots(1, 2, figsize=(14, 10))\n",
        "arr[0].imshow(img)\n",
        "arr[0].set_title('Image')\n",
        "arr[1].imshow(mask[:, :, 0])\n",
        "arr[1].set_title('Segmentation')"
      ],
      "metadata": {
        "id": "MbtTNvkxczf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Process images\n",
        "\n",
        "def process_path(image_path, mask_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=3)\n",
        "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
        "    return img, mask\n",
        "\n",
        "def preprocess(image, mask):\n",
        "    input_image = tf.image.resize(image, (96, 128), method='nearest')\n",
        "    input_mask = tf.image.resize(mask, (96, 128), method='nearest')\n",
        "\n",
        "    return input_image, input_mask\n",
        "\n",
        "image_ds = dataset.map(process_path)\n",
        "processed_image_ds = image_ds.map(preprocess)"
      ],
      "metadata": {
        "id": "llsda2BqdC2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*U-net*"
      ],
      "metadata": {
        "id": "l0p4u8ebcuEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(inputs=None, n_filters=32, dropout_prob=0, max_pooling=True):\n",
        "\n",
        "  conv = Conv3D(n_filters, # Number of filters\n",
        "                3,   # Kernel size   \n",
        "                activation='relu',\n",
        "                padding='same',\n",
        "                kernel_initializer='he_normal')(inputs)\n",
        "  conv = Conv3D(n_filters, # Number of filters\n",
        "                3,   # Kernel size\n",
        "                activation='relu',\n",
        "                padding='same',\n",
        "                kernel_initializer='he_normal')(conv)\n",
        "\n",
        "  if dropout_prob > 0:\n",
        "    conv = Dropout(dropout_prob)(conv)\n",
        "\n",
        "  if max_pooling:\n",
        "    next_layer = MaxPooling3D((2,2,2))(conv)       \n",
        "  else:\n",
        "    next_layer = conv\n",
        "      \n",
        "  skip_connection = conv\n",
        "  \n",
        "  return next_layer, skip_connection"
      ],
      "metadata": {
        "id": "MPfvUTzSculu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsampling(expansive_input, contractive_input, n_filters=32):\n",
        "\n",
        "  up = Conv3DTranspose(\n",
        "                n_filters,\n",
        "                3,    # Kernel size\n",
        "                strides=2,\n",
        "                padding='same')(expansive_input)\n",
        "  merge = concatenate([up, contractive_input], axis=3)\n",
        "  conv = Conv3D(n_filters,\n",
        "                3,     # Kernel size\n",
        "                activation='relu',\n",
        "                padding='same',\n",
        "                kernel_initializer='he_normal')(merge)\n",
        "  conv = Conv3D(n_filters,\n",
        "                3,   # Kernel size\n",
        "                activation='relu',\n",
        "                padding='same',\n",
        "                kernel_initializer='he_normal')(conv)\n",
        "  \n",
        "  return conv"
      ],
      "metadata": {
        "id": "1lYVYtp8eWuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################## CHANGE INPUT SIZE ################################\n",
        "\n",
        "def unet(input_size=(96, 128, 3), n_filters=32, n_classes=23):\n",
        "\n",
        "  inputs = Input(input_size)\n",
        "\n",
        "  cblock1 = conv(inputs, n_filters)\n",
        "\n",
        "  cblock2 = conv(cblock1[0], 2*n_filters)\n",
        "  cblock3 = conv(cblock2[0], 4*n_filters)\n",
        "  cblock4 = conv(cblock3[0], 8*n_filters, dropout_prob=0.3) \n",
        "  cblock5 = conv(cblock4[0], 16*n_filters, dropout_prob=0.3, max_pooling=False) \n",
        "\n",
        "  ublock6 = upsampling(cblock5[0], cblock4[1],  n_filters * 8)\n",
        "  ublock7 = upsampling(ublock6, cblock3[1],  n_filters * 4)\n",
        "  ublock8 = upsampling(ublock7, cblock2[1],  n_filters * 2)\n",
        "  ublock9 = upsampling(ublock8, cblock1[1],  n_filters)\n",
        "\n",
        "  conv9 = Conv3D(n_filters,\n",
        "                3,\n",
        "                activation='relu',\n",
        "                padding='same',\n",
        "                kernel_initializer='he_normal')(ublock9)\n",
        "\n",
        "  conv10 = Conv3D(n_classes, 1, padding='same')(conv9)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "tqBiJwA3e7-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Main*"
      ],
      "metadata": {
        "id": "QfpyB-vbfykM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################## CHANGE INPUT SIZE ################################\n",
        "\n",
        "img_height = 96\n",
        "img_width = 128\n",
        "num_channels = 3\n",
        "\n",
        "model = unet((img_height, img_width, num_channels))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HyWaj3QEfjK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Loss Function*"
      ],
      "metadata": {
        "id": "Wo1wDcddgQFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sgzFc-8CgJzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Train*"
      ],
      "metadata": {
        "id": "3tOCO83TgriN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40\n",
        "VAL_SUBSPLITS = 5\n",
        "BUFFER_SIZE = 500\n",
        "BATCH_SIZE = 32\n",
        "processed_image_ds.batch(BATCH_SIZE)\n",
        "train_dataset = processed_image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "print(processed_image_ds.element_spec)\n",
        "model_history = unet.fit(train_dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "5LjnVYSLgVWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Predicted Masks*"
      ],
      "metadata": {
        "id": "GIFOuHQHg2FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ],
      "metadata": {
        "id": "bSZ-mDo2g1pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model Accuracy*"
      ],
      "metadata": {
        "id": "Y2pdfAa1hCOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(model_history.history[\"accuracy\"])"
      ],
      "metadata": {
        "id": "nsSqMAXzhB9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example Results*"
      ],
      "metadata": {
        "id": "_jMgi9cBhU4h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "poW28bePhIDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}